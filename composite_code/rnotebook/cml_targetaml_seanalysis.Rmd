---
title: "Discovery of Acute Myeloid Leukemia Biomarkers using Ensemble Machine Learning"
output: html_notebook
---

```{r setup, include=FALSE}
require(knitr)
# knitr::opts_knit$set(root.dir = '~/Documents/GitHub/RNAseq_Cancer_Biomarkers/')
knitr::opts_chunk$set(tidy.opts=list(width.cutoff=50),
                      tidy=TRUE, 
                      fig.align='center', 
                      fig.height=5, 
                      fig.width=8, 
                      dpi = 600,
                      echo=FALSE,
                      eval=TRUE)
options(stringsAsFactors = FALSE)
```

```{r libraries, eval=TRUE, echo=FALSE}
# dependency libraries
library(plyr)
library(mlr)
library(magrittr)
library(ggplot2)
library(EnsDb.Hsapiens.v75)
library(glmnet)
library(ROSE)
library(knitr)
library(stringr)
library(dplyr)
library(tibble)
library(tidyr)
library(limma)
library(edgeR)
library(MLSeq)
library(DESeq2)
library(xlsx)
library(VennDiagram)
library(SummarizedExperiment)
library(GenomicRanges)
library(circlize)
library(reshape2)

```

```{r mlfunctions, eval=FALSE}
# ML functions
# Lasso
glm.binom <- function(x, y, df, ref="No", train.names=NULL, test.names=NULL, 
                      standardize=FALSE, splitIntoTrain=FALSE){
  # credit: Jenny Smith
  library(glmnet)
  #df is the matrix with the response and  gene expression. Patients as rownames.
  #x is the character vector of column names for genes 
  #y is the character vector of column names for the classifier 
  #train is a chacter vector of patient IDs
  #test is a chacter vector of Patient IDs. 
  
  response <- y
  predictors <- x
  
  #check this that referece should be the first level in glmnet package
  #Set-up the x and y matrices 
  y <- factor(df[,y])
  y <- relevel(y,ref = ref)  %>% set_names(rownames(df))
  x <- as.matrix(df[,x]) #NOTE: for categorical predictors data, should use model.matrix 
  
  
  if (any(c(is.na(y), is.na(x)))) {
    print("There Are Missing Values.")
    return(list(x=x,y=y))
  }
  
  #Check the reference level of the response.
  contrast <- contrasts(y)

  if(splitIntoTrain){
    #Use validation set approach. split observations into approx. equal groups.
    set.seed(1)  
    train <- sample(c(TRUE,FALSE), nrow(x), replace = TRUE)
    test <- (!train)
  
    train.names <- rownames(df)[train]
    test.names <- rownames(df)[test]
  }


  #grid of lambda values to test.
  grid <- 10^ seq(10,-2, length=100)
    
  #training model.
  fit <- glmnet(x[train.names,], y[train.names],
                family = "binomial",
                alpha=1,
                standardize = standardize, 
                lambda = grid, 
                intercept = FALSE)

  #use cross-validation on the training model.CV only for lambda
  set.seed(2019) 
  cv.fit <- cv.glmnet(x[train.names,], y[train.names],
                  family = "binomial",
                  type.logistic="modified.Newton", 
                  standardize = standardize,
                  lambda = grid,
                  alpha=1,
                  nfolds = length(train.names), #LOOCV 
                  type.measure = "class", 
                  intercept = FALSE)

  #Select lambda min.
  lambda.min <- cv.fit$lambda.min

  #predict the classes
  pred.class <- predict(fit, newx = x[test.names,], type="class", s=lambda.min)

  #find the test error
  tab <- table(pred.class,y[test.names])
  testError <- mean(pred.class != y[test.names]) #how many predicted classes were incorrect

  #Fit the full dataset.
  final <- glmnet(x, y,family = "binomial",
                  standardize = standardize, 
                  lambda = grid,
                  alpha = 1,
                  intercept = FALSE)

  #Extract the coefficients
  coef <- predict(final, type="coefficients", s=lambda.min)
  idx <- which(coef != 0)
  nonZero <- coef[idx,]

  #Results 
  list <- list(train.names, test.names, contrast, fit, cv.fit,tab,testError, final, nonZero)
  names(list) <- c("training.set", "testing.set","contrast", "train.fit",
                   "cv.fit", "confusionMatrix","test.error", "final.model", "nonzero.coef")
  return(list)
  
}
# SVM
runSVM <- function(seed,kerneltype="linear",trainset,trainclasses,
                   testset,testclasses, weightfilt=FALSE){
  # credit : Sean Maden
  # run SVM optimization
  # Arguments
  #   * seed : set seed (int) for randomization
  #   * kerneltype : (str) type of kernel for SVM, either 'linear' or 'gaussian'
  #   * trainset : training dataset (excluding sample classes)
  #   * trainclasses : classes for training sampels (vector) with 1:1 correspondence 
  #       with trainset rows
  #   * testset : test data (data frame or matrix), excluding classes
  #   * testclasses : classes for test samples (vector), with 1:1 row:pos correspondence
  #   * weightfilt : (FALSE or numeric) top percentage weights to use in model 
  #       (if FALSE, then all weights used) 
  # Returns
  #   * rl (list) : list containing model fitted, predictions, and performacne metrics
  require(e1071); require(ROCR)
  rl <- list(); str.options <- ""
  set.seed(seed)
  ndtr <- trainset
  ndte <- testset
  ndtr.classes <- trainclasses
  ndte.classes <- testclasses
  
  # train svm model
  svm_model <- svm(as.factor(ndtr.classes)~., 
                   data=ndtr, 
                   method="C-classification", 
                   kernel=kerneltype)
  weightsvect <- ndtr.weights <- t(svm_model$coefs) %*% svm_model$SV
  if(weightfilt){
    str.options <- c(str.options,paste0("weight filt = ",weightfilt))
    # order training data on relative weights
    ndtr.weightsort <- ndtr[,rev(order(abs(ndtr.weights)))]
    # select only top proportion weights
    nweight.col = round(ncol(ndtr.weightsort)*weightfilt,0)
    ndtr.weightfilt <- ndtr.weightsort[,c(1:nweight.col)]
    str.options <- c(str.options,paste("cols_retained:",colnames(ndtr.weightfilt),collapse=";"))
    # redefine training set, rerun SVM optimization
    ndtr <- ndtr.weightfilt
    svm_model <- svm(as.factor(ndtr.classes)~., 
                     data=ndtr, 
                     method="C-classification", 
                     kernel=kerneltype)
  } else{
    str.options <- c(str.options,"no weight filt")
  }
  pred_train <- predict(svm_model, ndtr, decision.values = TRUE)
  pred_test <- predict(svm_model, ndte, decision.values = TRUE)
  # get performance metrics
  pred <- prediction(as.numeric(attr(pred_test,"decision.values")),ndte.classes)
  perf <- performance(pred,"tpr","fpr")
  ppred <- pred_test[pred_test==1]; 
  tppred <- ndte.classes[pred_test==1]
  ppred <- as.numeric(as.character(ppred))
  testprec <- length(ppred[ppred==tppred])/length(ppred) # test precision
  rposi <- ndte.classes==1
  rtpred <- ndte.classes[rposi]; 
  rppred <- pred_test[rposi]
  rppred <- as.numeric(as.character(rppred))
  testrec <- length(rppred[rppred==1])/length(rppred) # test recall
  
  # return model, pred's, and performance metrics
  rl <- list(str.options,
             svm_model,
             weightsvect,
             pred_train,
             pred_test,
             perf,
             tppred,
             testprec,
             testrec)
  names(rl) <- c("options_string",
                 "svm_model",
                 "weightsvect",
                 "predictions_train",
                 "predictions_test",
                 "performance_test",
                 "TPR_test",
                 "precision_test",
                 "recall_test"
  )
  return(rl)
  
}

```

```{r utilityfunctions, eval=FALSE}
# utilities for data summaries and visualization

#==============================
# differential gene expression
#==============================
voom_DE <- function(counts.df, ref, pheno){
  # credit: Jenny Smith
  #counts.df is a dataframe with count data, with genes as rownames
  #pheno is a character vector with patient IDs as names, and the status for each in each group(eg pos,neg)
  require(edgeR)
  library(limma)
  
  #ensure correct order for both expn and counts.df
  samples <- intersect(names(pheno), colnames(counts.df))
  pheno <- pheno[samples]
  counts.df <- counts.df[,samples]
  
  
  groups <- unique(pheno)
  groups <- c(groups[groups != ref], ref) #order so that reference is second 
  pheno.f <- factor(pheno, levels=groups)

  dge <- DGEList(counts = counts.df, group = pheno.f)

  keep.dge <- rowSums(cpm(dge) >= 1) > (0.05*ncol(counts.df)) #5% of samples with CPM >= 1
  dge <- dge[keep.dge,]
  dge <- calcNormFactors(dge)

  design <- model.matrix(~0 + pheno.f, data=dge$samples)
  colnames(design) <- levels(pheno.f)
  cont.matrix <- makeContrasts(contrasts = paste(groups, collapse = "-"), levels = design)
  
  
  v.lv <- voom(dge, design, plot = FALSE)
  

  fit <- lmFit(v.lv, design)
  fit <- contrasts.fit(fit, contrasts = cont.matrix)
  fit <- eBayes(fit)
  table <- topTable(fit, number = 20000, p.value=0.05, adjust.method="BH", sort.by="P",lfc=1)
  


  list <- list(design, v.lv, fit, table)
  names(list) <- c("desingMatrix", "voomTransformation", "fit", "DEGs")
  return(list)
}

#=================
# categorize DEGs
#=================
catExpnData <- function(filenames,regex, cols, header=FALSE,removeFirstLine=FALSE, sep="\t"){
  #credit: Jenny Smith
  # Purpose: Concatenate the expression data-sets downloaded from TCGA/TARGET from GDC or any patient level data
  #eg. each individual patient has a single expression-file 
  
  library(magrittr)
  options(stringsAsFactors = FALSE)
  #filenames is a character vector of all filenames. 
  #regex is a string with the pattern to extract the patient ID , eg "^.+(Kasumi|MV4)", from filenames 
  #cols is the character vector or numeric vector of the columns to select and concatenate. 
  
  extract_cols <-function(filename,cols,rmFirstLine=FALSE){
    
    if(all(rmFirstLine & header)){
      aFile <- readLines(filename)[-1] #remove first line with extra info. 
      aFile <- str_split_fixed(aFile, pattern = "\t",n = length(cols)) %>% #split into a matrix
        set_colnames(.[1,] ) %>%  #set colnames from the first line 
        .[-1, ] #remove the header row from matrix
    }else{
      aFile <- read.delim(filename, sep=sep, header=header, as.is=TRUE)
    }
    
    output <- list()
    for ( k in 1:length(cols)){
      colname <- cols[k]
      col <- aFile[,colname]
      output[[colname]] <- col
    }
    return(output)
  }
  
  combineColumns <- function(extract_cols.res,colname){
    sapply(extract_cols.res, '[[', colname)
  }
  
  
  IDs <- gsub(regex, "\\1", filenames)
  
  columns <- lapply(filenames,extract_cols,cols=cols, rmFirstLine=removeFirstLine) %>%
    set_names(IDs)
  
  catedMatrices <- lapply(cols, combineColumns, extract_cols.res=columns)  %>%
    set_names(cols)
  
  
  return(catedMatrices)
}

#==============
# volcano plot
#==============
volcano_plot <- function(fit, cut.off=4, label.offset=0.5){
  # credit : Jenny Smith
  df <- data.frame(logFC=fit$coefficients[,1],
                   pValue=fit$p.value[,1],
                   FDR=p.adjust(fit$p.value[,1], method="BH"),
                   MeanExpression=fit$Amean) %>%
      rownames_to_column("Gene") %>%
      mutate(Neg.Log10.P= -log10(pValue),
             DEGs.Groups=case_when(
                  logFC > 1.0 & pValue < 0.05 ~ "FC Greater than 2",
                  logFC < -1.0 & pValue < 0.05 ~ "FC Less than 2",
                  TRUE ~ "Not Significant FC"))

  
  #Select differentially expressed genes to highlight in the plot. 
  ToHighlight <- df[abs(df$logFC) > cut.off & df$FDR < 0.05, "Gene"] 
  idx <- which(abs(df$logFC) > cut.off & df$FDR < 0.05)
  
  vplot <- ggplot(df, aes(x=logFC, y=Neg.Log10.P)) + 
    geom_point(data = filter(df, DEGs.Groups == "Not Significant FC"), 
               mapping = aes(x=logFC, y=Neg.Log10.P, color=DEGs.Groups), alpha=0.65)  +
    
    geom_point(data= filter(df, grepl("2", DEGs.Groups)), 
               mapping = aes(x=logFC, y=Neg.Log10.P, color=DEGs.Groups)) +
    
    geom_vline(xintercept=c(-1,1)) +
    geom_hline(yintercept = -log10(0.05)) +
    
    scale_color_manual(values=c("FC Greater than 2"="red", 
                                "FC Less than 2"="blue",
                                "Not Significant FC"="lightgrey")) +
    
    theme(plot.title = element_text(hjust = 0.5, size = 20),
          panel.background = element_rect(fill="white"),
          panel.grid.major = element_blank(),
          panel.grid.minor = element_blank(),
          panel.border = element_rect(color = "black", fill=NA),
          axis.text = element_text(color = "black"),
          axis.text.x = element_text(angle = 0,hjust=0.5,vjust = 0.5, size = 26),
          axis.text.y = element_text(size = 25),
          axis.title = element_text(size = 30),
          plot.margin = margin(2,2,2,2, unit = "mm")) +
    
    geom_text(aes(x=logFC+label.offset, y=Neg.Log10.P, label=ToHighlight),size=3.5,
              data=df[idx, ])
 

  return(vplot)
  
}

```

```{r globals_and_loadobj}
# define globals
sys.sep = "/"
data.dir = "data"
seobj.dir = "seobjects"
figs.dir = "figures"

# summarized experiment object names
countsseset.name <- "seset_genecounts_targetaml.rda"
tmmseset.name <- "seset_genetmmfilt_targetaml.rda"
degseset.name <- "seset_degseahack_targetaml.rda"
degfiltset.name <- "sesetfilt_degseahack_targetaml.rda"
maeobj.name <- "mae_targetaml.rda"

# figure filenames
cormap.tile.name <- "cortest_tile.jpg"
cormap.tri.name <- "cortest_triangle.jpg"
hmdeg.name <- "hmdeg_targetaml.jpg"
hmdeg.rowanno.name <- "hmdeg_rowanno_targetaml.jpg"
hmdeg.traintest.name <- "hmdeg_traintest_targetaml.jpg"

# lasso results objects and figures
lasso1.nocofilt.name <- "lasso_rep1_nocofilt_resultlist.rda"
lasso2.cofilt.name <- "lasso_rep2_cofilt1_resultlist.rda"
lasso3.cofilt2.name <- "lasso_rep3_cofilt2_resultlist.rda"
lasso1.figure.name <- "lasso_rep1_features.jpg"

# random forest (noboost) objects and figures
rf.resultlist.name <- "rfiter-noboost_resultlist_smtest.rda"
rf.plot2k.name <- "rf2k_featureimportance.jpg"
rf.plot5k.name <- "rf5k_featureimportance.jpg"
rf.plot10k.name <- "rf10k_featureimportance.jpg"
rf.plotcomposite.name <- "rfall_noboost_featureimportance.jpg"

# standard outputs table
stdtable.name <- "standardtable_mloutputs_summary.rda"

```

```{r load_seobject}
# Load the primary summarized experiment object for experiment
load(paste0(seobj.dir, sys.sep, degseset.name))

```

# Data Preparation
Summarized Experiment objects were created from TARGET AML clinical and RNA-seq data obtained from GDC. Gene counts from STAR 2-Pass alignment were converted using TMM into log counts-per-million scale. Differentially Expressed Genes were determined comparing classifier sample groups in the training sample subset only. Genes were pre-filered on whether at least 5 samples (set-wide) showing counts per mission greater than or equal to 1. 

# Data Summaries and Pre-filtering Samples with Risk Group Available
```{r summarizeclin, eval=TRUE, echo=TRUE}
# Summary statistics and prefiltering samples

#=========================
# summarise the se object
#=========================
message("dim se object")
dim(deg.seset)
# [1] 1984  145
message("table of risk group var")
table(deg.seset$Risk.group)
#     High      Low Standard  Unknown 
#       8       60       69        8
deg.seset$deg.risk <- ifelse(deg.seset$Risk.group=="Low", 0,
                             ifelse(deg.seset$Risk.group %in% c("Standard","High"),1,"NA"))
message("table of binarized risk group")
table(deg.seset$deg.risk)
# 0  1 NA 
# 60 77  8
message("table of risk group x binarized risk group")
table(deg.seset$deg.risk, deg.seset$Risk.group)
#       High Low Standard Unknown
#  0     0  60        0       0
#  1     8   0       69       0
#  NA    0   0        0       8

#=========================================
# pre-filter on available risk group data
#=========================================
degfilt.se <- deg.seset[,which(deg.seset$deg.risk %in% c(0,1))] # subset on deg risk group available
message("dim of filtered se object")
dim(degfilt.se)
# [1] 1984  137
# summarize gender and age at first diagnosis
message("table of gender x binarized risk")
table(degfilt.se$Gender,degfilt.se$deg.risk)
#           0  1
#   Female 29 40
#   Male   31 37
message("chisq test of gender x binarized risk")
chisq.test(table(degfilt.se$Gender,degfilt.se$deg.risk)) # p-value = 0.8044, gender evenly dist
degfilt.se$binom.age <- ifelse(degfilt.se$Age.at.Diagnosis.in.Days >= median(degfilt.se$Age.at.Diagnosis.in.Days), "old" ,"young")
message("table of binarized age-at-diag x binarized risk")
table(degfilt.se$binom.age,degfilt.se$deg.risk)
#         0  1
#  old   32 37
#  young 28 40
message("chisq results of binarized age-at-diag x binarized risk")
chisq.test(table(degfilt.se$binom.age,degfilt.se$deg.risk)) #  p-value = 0.6591, age evenly dist

save(degfilt.se, file=paste0("data/",degfiltset.name))

```

# Differentially Expressed Genes Summary
```{r summarizedeg, eval=TRUE, echo=TRUE}
# DEG summaries of gene set expr. data, classifier differences, etc.

allgenes <- rownames(degfilt.se)
genemeans.0 <- rowMeans(assay(degfilt.se[,degfilt.se$deg.risk==0]), na.rm=T)
genemeans.1 <- rowMeans(assay(degfilt.se[,degfilt.se$deg.risk==1]), na.rm=T)
lfc.deg <- log2((genemeans.1/(genemeans.0+0.01)))

message("summary of log2FC")
summary(lfc.deg)
# Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's 
# -8.4397 -1.0912 -0.6326 -0.4780  0.4111  7.9643     366

length(allgenes[which(lfc.deg < -4)]) # 27
length(allgenes[which(lfc.deg < -2)]) # 144
length(allgenes[which(lfc.deg < 0)]) # 1165
length(allgenes[which(lfc.deg > 0)]) # 412
length(allgenes[which(lfc.deg > 2)]) # 84
length(allgenes[which(lfc.deg > 4)]) # 14

#===========
# DEG plots
#===========
plot(density(lfc.deg))
hist(lfc.deg, breaks = 50)
plot(lfc.deg,-1*log10(rowData(degfilt.se)$p.unadj),
     xlab="Log2FC(NotLow_1/Low_0)",
     ylab="-1*log10(padj)")

```

# Heatmap Data Set Summaries
```{r degheatmap, eval=TRUE, echo=FALSE}
# Heatmap dataset summaries, full dataset and data subsets
require(ComplexHeatmap)
require(circlize)

#=======================
# Heatmap Test vs Train
#=======================
# all data
seset <- degfilt.se
hm_data <- as.matrix(assay(seset))
breaks=seq(min(hm_data),max(hm_data),0.1)
hmcol = colorRamp2(breaks, colorRampPalette(c("green","black","red"))(n=length(breaks)))

# train data
seset <- degfilt.se[,degfilt.se$exptset.seahack=="train"]
hm_data.train <- as.matrix(assay(seset)[!is.na(lfc.deg),])
# hm col annotations can contain complex layered output and colored annotaions
hm_colanno.train <- HeatmapAnnotation(show_legend = TRUE,
                                df = data.frame(sampletype=seset$deg.risk,
                                                gender=seset$Gender,
                                                binom.age=seset$binom.age), 
                                col = list(sampletype = c("0" =  "blue","1" = "red"),
                                           gender = c("Male"="black","Female"="gray"),
                                           binom.age = c("young"="pink","old"="green")),
                                name = "Sample Type",
                                annotation_height = unit(c(0.5, 0.5, 0.5), "cm"))
hm.train <- Heatmap(hm_data.train,
        col=hmcol,
        cluster_columns = TRUE,
        show_heatmap_legend = TRUE,
        top_annotation = hm_colanno.train,
        name="tmm_log_cpm",
        show_row_names = FALSE,
        show_column_names = FALSE,
        column_title = "Train", 
        column_dend_reorder = TRUE,
        row_dend_reorder = TRUE,
        heatmap_legend_param = list(color_bar = "continuous"),
        row_title = "DEGs")

# test data
seset <- degfilt.se[,degfilt.se$exptset.seahack=="test"]
hm_data.test <- as.matrix(assay(seset)[!is.na(lfc.deg),])
# hm col annotations can contain complex layered output and colored annotaions
hm_colanno.test <- HeatmapAnnotation(show_legend = TRUE,
                                df = data.frame(sampletype=seset$deg.risk,
                                                gender=seset$Gender,
                                                binom.age=seset$binom.age), 
                                col = list(sampletype = c("0" =  "blue","1" = "red"),
                                           gender = c("Male"="black","Female"="gray"),
                                           binom.age = c("young"="pink","old"="green")),
                                name = "Sample Type",
                                annotation_height = unit(c(0.5, 0.5, 0.5), "cm"))
hm.test <- Heatmap(hm_data.test,
        col=hmcol,
        cluster_columns = TRUE,
        show_heatmap_legend = TRUE,
        top_annotation = hm_colanno.test,
        name="tmm_log_cpm",
        show_row_names = FALSE,
        show_column_names = FALSE,
        column_title = "Test", 
        column_dend_reorder = TRUE,
        row_dend_reorder = TRUE,
        heatmap_legend_param = list(color_bar = "continuous"),
        row_title = "DEGs")
draw(hm.test+hm.train)

jpeg(paste0(figs.dir, sys.sep, hmdeg.traintest.name), 10, 5, units="in", res=400)
draw(hm.test+hm.train)
dev.off()

#==================
# Heatmap All Data
#==================
# Take normally dist data as heatmap matrix
hm_data <- as.matrix(assay(degfilt.se))
# hm col annotations can contain complex layered output and colored annotaions
hm_colanno <- HeatmapAnnotation(show_legend = TRUE,
                                df = data.frame(sampletype=degfilt.se$deg.risk,
                                                gender=degfilt.se$Gender,
                                                binom.age=degfilt.se$binom.age,
                                                subset=degfilt.se$exptset.seahack), 
                                col = list(sampletype = c("0" =  "blue","1" = "red"),
                                           gender = c("Male"="black","Female"="gray"),
                                           binom.age = c("young"="pink","old"="green"),
                                           subset = c("test" = "orange", "train" = "purple")),
                                name = "Sample Type",
                                annotation_height = unit(c(0.5, 0.5, 0.5, 0.5), "cm"))
breaks=seq(min(hm_data),max(hm_data),0.1)
hmcol = colorRamp2(breaks,colorRampPalette(c("green","black","red"))(n=length(breaks)))
hm <- Heatmap(hm_data,
        col=hmcol,
        cluster_columns = TRUE,
        show_heatmap_legend = TRUE,
        top_annotation = hm_colanno,
        name="log2_tmm_count",
        show_row_names = FALSE,
        show_column_names = FALSE,
        column_title = "Samples", 
        column_dend_reorder = TRUE,
        row_dend_reorder = TRUE,
        heatmap_legend_param = list(color_bar = "continuous"),
        row_title = "DEGs")

#hm

jpeg(paste0(figs.dir, sys.sep, hmdeg.name), 8, 7, units="in", res=400)
hm
dev.off()

# transverse heatmap with log2FC

```

# Machine Learning: Model Fitting and Assessment

## Standardized Output Table
Results from iterations of model fitting and feature weighting analyses were stored in a single table of all DEGs tested.

```{r standard_table_output}
# Initialize the standard table to hold all test results
standtable = as.data.frame(rowData(degfilt.se))

```

## Support Vector Machines
The support vector machines (SVM) algorithm was run with the following parameters...
```{r svm}
```

## Lasso
Lasso was run with the following parameters...
```{r lasso_nofilt}
# Run lasso with glmnet
#df is the matrix with the response and  gene expression. Patients as rownames.
  #x is the character vector of column names for genes 
  #y is the character vector of column names for the classifier 
  #train is a chacter vector of patient IDs
  #test is a chacter vector of Patient IDs. 
#Mod.RG <- glm.binom(x = grep("^ENSG",  colnames(RG.GLM.df), value=TRUE),
#                    y="RiskGroup.Class",
#                    df=RG.GLM.df,
#                    train.names = RG.GLM.df$TARGET.USI[RG.GLM.df$train_test_set=="Train"],
#                    test.names = RG.GLM.df$TARGET.USI[RG.GLM.df$train_test_set=="Test"])

#lasso.fit.rg <- glm.binom(x = gene.names, y = var.classifier, df = glm.df, 
#                          train.names = trainset.names, test.names = testset.names)

#---------------
# SM lasso test
#---------------
repnum = 0
seset <- degfilt.se
dim(seset) # [1] 1937  137

gene.names = as.character(rownames(rowData(seset)))
var.classifier = seset$deg.risk

glm.df = t(assay(seset))
trainset.names = colnames(assay(seset[,seset$exptset.seahack=="train"]))
testset.names = colnames(assay(seset[,seset$exptset.seahack=="test"]))
#----------------------------
df = glm.df 
train.names = trainset.names
test.names = testset.names
#----------------------------
response <- var.classifier
predictors <- gene.names
y <- factor(response); names(y) <- colnames(assay(seset)) # response var obj
x = df[,colnames(df) %in% predictors] # genes of interest
dim(x)
contrast <- contrasts(y)
grid <- 10^ seq(10,-2, length=100)
standardize = FALSE
fit <- glmnet(x[train.names,], y[train.names], family = "binomial", alpha=1, 
              standardize = standardize, lambda = grid, intercept = FALSE)
# use cross-validation on the training model.CV only for lambda
if(repnum==0){
  newseed = 2019
  set.seed(newseed) 
} else{
  newseed = sample(seq(0,10000,1),1)
  set.seed(newseed)
}
cv.fit <- cv.glmnet(x[train.names,], y[train.names], family = "binomial",
                    type.logistic="modified.Newton", standardize = standardize,
                    lambda = grid, alpha=1, nfolds = length(train.names), #LOOCV 
                    type.measure = "class", intercept = FALSE)
#Select lambda min.
lambda.min <- cv.fit$lambda.min
#predict the classes
pred.class <- predict(fit, newx = x[test.names,], type="class", s=lambda.min)
#find the test error
tab <- table(pred.class,y[test.names])
testError <- mean(pred.class != y[test.names]) #how many predicted classes were incorrect
#Fit the full dataset.
final <- glmnet(x, y,family = "binomial", standardize = standardize, 
                lambda = grid, alpha = 1, intercept = FALSE)
#Extract the coefficients
coef <- predict(final, type="coefficients", s=lambda.min)
idx <- which(!as.numeric(coef)==0)
nonZero <- coef[idx,]
#Results 
smlasso.list <- list(train.names, test.names, contrast, fit, 
                     cv.fit, tab, testError, final, nonZero, newseed)
names(smlasso.list) <- c("training.set", "testing.set","contrast", "train.fit",
                 "cv.fit", "confusionMatrix","test.error", "final.model", 
                 "nonzero.coef", "seednum")

save(smlasso.list, file = paste0(data.dir, sys.sep, lasso1.nocofilt.name))

#if(repnum==0){
#  save(smlasso.list, file = paste0(data.dir, sys.sep, lasso1.nocofilt.name))
#} else{
#  save(smlasso.list, file = paste0(data.dir, sys.sep, smlasso.resultlist.repname, repnum, #".rda"))
#}

#======================================
# Append Results to Standardized Table
#======================================
coefgenes_dfsym = names(smlasso.list$nonzero.coef)
ncoefgenes_dfsym = rownames(seset[!rownames(seset) %in% coefgenes_dfsym,])

cgl = smlasso.list$nonzero.coef
coeffvect = c(as.numeric(cgl), 
              rep(0, nrow(seset[!rownames(seset) %in% names(cgl)]))
              )
names(coeffvect) <- c(names(cgl), rownames(seset[!rownames(seset) %in% names(cgl)]))
coeffvect = coeffvect[order(match(names(coeffvect), rownames(standtable)))]
identical(names(coeffvect), rownames(standtable))

standtable$lasso_coeff <- coeffvect

save(standtable, file = paste0(data.dir, sys.sep, stdtable.name))

#=========================
# Plot lasso coefficients
#=========================
# goi (retained genes with nonzero coeff)
lasso.goi <- names(smlasso.list$nonzero.coef)
# gene symbols
lasso.goi.symbols <- rowData(deg.seset[lasso.goi,])$hgnc_symbol
#data.frame(lasso.goi, lasso.goi.symbols, as.numeric(smlasso.list$nonzero.coef))

# make barplot
bpdat <- smlasso.list$nonzero.coef
gene.order <- order(bpdat)
bpdat <- bpdat[gene.order]
bp.gene.symbols <- paste0(lasso.goi[gene.order],"; ",lasso.goi.symbols[gene.order])
bpcol <- ifelse(bpdat<0,"blue","red")

jpeg(paste0(figs.dir, sys.sep, lassocoeff.figure.name),8,8,units="in",res=400)

par(oma=c(10,2,2,2))
barplot(bpdat, col=bpcol, 
        names.arg = bp.gene.symbols,
        cex.names = 0.75,
        las = 2, 
        ylim = c(-0.1,0.3),
        main = "Lasso Features")
mtext(text="Coefficient",side=2,line=0,outer=TRUE,cex=1)
mtext(text="Gene Symbol",side=1,line=7,outer=TRUE,cex=1)

dev.off()

```

## Lasso with Coefficients Filter
Lasso was repeated after first excluding genes selected in the first lasso run, or genes having non-zero coefficients after the first round of lasso. Correlatedness of genes selected in these two runs was then analyzed.

```{r lasso_rep2_cofilt1}
# Re-run lasso, excluding selected genes from first lasso run
genes.exclude = names(smlasso.list$nonzero.coef)
repnum = 0
se.subset <- degfilt.se[!rownames(degfilt.se) %in% genes.exclude,]
dim(se.subset) # [1] 1971  137
gene.names = as.character(rownames(rowData(se.subset)))
var.classifier = se.subset$deg.risk
glm.df = t(assay(se.subset))
trainset.names = colnames(assay(se.subset[,se.subset$exptset.seahack=="train"]))
testset.names = colnames(assay(se.subset[,se.subset$exptset.seahack=="test"]))
df = glm.df 
train.names = trainset.names
test.names = testset.names
response <- var.classifier
predictors <- gene.names
y <- factor(response); names(y) <- colnames(assay(se.subset)) # response var obj
x = df[,colnames(df) %in% predictors] # genes of interest
dim(x)
contrast <- contrasts(y)
grid <- 10^ seq(10,-2, length=100)
standardize = FALSE
fit <- glmnet(x[train.names,], y[train.names], family = "binomial", alpha=1, 
              standardize = standardize, lambda = grid, intercept = FALSE)
# use cross-validation on the training model.CV only for lambda
newseed = 2019
set.seed(newseed) 

cv.fit <- cv.glmnet(x[train.names,], y[train.names], family = "binomial",
                    type.logistic="modified.Newton", standardize = standardize,
                    lambda = grid, alpha=1, nfolds = length(train.names), #LOOCV 
                    type.measure = "class", intercept = FALSE)
#Select lambda min.
lambda.min <- cv.fit$lambda.min
pred.class <- predict(fit, newx = x[test.names,], type="class", s=lambda.min)
tab <- table(pred.class,y[test.names])
testError <- mean(pred.class != y[test.names]) #how many predicted classes were incorrect
# fit the full dataset.
final <- glmnet(x, y,family = "binomial", standardize = standardize, 
                lambda = grid, alpha = 1, intercept = FALSE)
# extract coefficients
coef <- predict(final, type="coefficients", s=lambda.min)
idx <- which(!as.numeric(coef)==0)
nonZero <- coef[idx,]
# store results list 
lasso.rep2.cofilt.resultlist <- list(train.names, test.names, contrast, fit, 
                     cv.fit, tab, testError, final, nonZero, newseed)
names(lasso.rep2.cofilt.resultlist) <- c("training.set", "testing.set","contrast", "train.fit",
                 "cv.fit", "confusionMatrix","test.error", "final.model", 
                 "nonzero.coef", "seednum")

save(lasso.rep2.cofilt.resultlist, file=paste0(data.dir, sys.sep, lasso_rep2_cofilt1_resultlist.rda))

#======================================
# Append Results to Standardized Table
#======================================
lr <- lasso.rep2.cofilt.resultlist
coefgenes_dfsym = names(lr$nonzero.coef)
ncoefgenes_dfsym = rownames(seset[!rownames(seset) %in% coefgenes_dfsym,])
cgl = lr$nonzero.coef
coeffvect = c(as.numeric(cgl), 
              rep(0, nrow(seset[!rownames(seset) %in% names(cgl)]))
              )
names(coeffvect) <- c(names(cgl), rownames(seset[!rownames(seset) %in% names(cgl)]))
coeffvect = coeffvect[order(match(names(coeffvect), rownames(standtable)))]
identical(names(coeffvect), rownames(standtable))
standtable$lasso_rmCoeff <- coeffvect

save(standtable, file = paste0(data.dir, sys.sep, stdtable.name))

```

```{r lasso_rep3_cofilt2}
# Re-run lasso, excluding selected genes from first lasso run

# load standard table
load("data/standardtable_mloutputs_summary.rda")
# get genes to rm
genes.exclude <- rownames(standtable[!standtable$lasso_coeff==0 | !standtable$lasso_rmCoeff==0,])

repnum = 0
se.subset <- degfilt.se[!rownames(degfilt.se) %in% genes.exclude,]
dim(se.subset) # [1] 1956  137
gene.names = as.character(rownames(rowData(se.subset)))
var.classifier = se.subset$deg.risk
glm.df = t(assay(se.subset))
trainset.names = colnames(assay(se.subset[,se.subset$exptset.seahack=="train"]))
testset.names = colnames(assay(se.subset[,se.subset$exptset.seahack=="test"]))
df = glm.df 
train.names = trainset.names
test.names = testset.names
response <- var.classifier
predictors <- gene.names
y <- factor(response); names(y) <- colnames(assay(se.subset)) # response var obj
x = df[,colnames(df) %in% predictors] # genes of interest
dim(x)
contrast <- contrasts(y)
grid <- 10^ seq(10,-2, length=100)
standardize = FALSE
fit <- glmnet(x[train.names,], y[train.names], family = "binomial", alpha=1, 
              standardize = standardize, lambda = grid, intercept = FALSE)
# use cross-validation on the training model.CV only for lambda
newseed = 2019
set.seed(newseed) 

cv.fit <- cv.glmnet(x[train.names,], y[train.names], family = "binomial",
                    type.logistic="modified.Newton", standardize = standardize,
                    lambda = grid, alpha=1, nfolds = length(train.names), #LOOCV 
                    type.measure = "class", intercept = FALSE)
#Select lambda min.
lambda.min <- cv.fit$lambda.min
pred.class <- predict(fit, newx = x[test.names,], type="class", s=lambda.min)
tab <- table(pred.class,y[test.names])
testError <- mean(pred.class != y[test.names]) #how many predicted classes were incorrect
# fit the full dataset.
final <- glmnet(x, y,family = "binomial", standardize = standardize, 
                lambda = grid, alpha = 1, intercept = FALSE)
# extract coefficients
coef <- predict(final, type="coefficients", s=lambda.min)
idx <- which(!as.numeric(coef)==0)
nonZero <- coef[idx,]
# store results list 
lasso.rep3.cofilt2.resultlist <- list(train.names, test.names, contrast, fit, 
                     cv.fit, tab, testError, final, nonZero, newseed)
names(lasso.rep3.cofilt2.resultlist) <- c("training.set", "testing.set","contrast", "train.fit",
                 "cv.fit", "confusionMatrix","test.error", "final.model", 
                 "nonzero.coef", "seednum")

save(lasso.rep3.cofilt2.resultlist, file=paste0(data.dir, sys.sep, lasso3.cofilt2.name))

#======================================
# Append Results to Standardized Table
#======================================
lr <- lasso.rep3.cofilt2.resultlist
coefgenes_dfsym = names(lr$nonzero.coef)
ncoefgenes_dfsym = rownames(degfilt.se[!rownames(degfilt.se) %in% coefgenes_dfsym,])
cgl = lr$nonzero.coef
coeffvect = c(as.numeric(cgl), 
              rep(0, nrow(degfilt.se[!rownames(degfilt.se) %in% names(cgl)]))
              )
names(coeffvect) <- c(names(cgl), rownames(degfilt.se[!rownames(degfilt.se) %in% names(cgl)]))
coeffvect = coeffvect[order(match(names(coeffvect), rownames(standtable)))]
identical(names(coeffvect), rownames(standtable))
standtable$lasso_rep3_cofilt2 <- coeffvect

save(standtable, file = paste0(data.dir, sys.sep, stdtable.name))

```

## Random Forest without Boosting
Random forest was run without boosting, varying the number of trees from 2,000 to 10,000.

```{r random_forest_noboost}
# Random forest model fitting
# varying trees to be 2k, 5k, 10k
require(randomForest)
set.seed(20)

# select training data
rfdat.train <- as.data.frame(t(assay(deg.seset[,deg.seset$exptset.seahack=="train"])))
# append binom classes
rfdat.train$class <- as.factor(deg.seset[,deg.seset$exptset.seahack=="train"]$deg.risk)
# form test set as with train set
rfdat.test <- as.data.frame(t(assay(deg.seset[,deg.seset$exptset.seahack=="test"])))
#rfdat.test$class <- as.factor(deg.seset[,deg.seset$exptset.seahack=="test"]$deg.risk)

rf2k <- randomForest(class ~ .,
                     data = rfdat.train,
                     ntree = 2000,
                     proximity = TRUE)

rf5k <- randomForest(class ~ .,
                     data = rfdat.train,
                     ntree = 5000,
                     proximity = TRUE)

rf10k <- randomForest(class ~ .,
                     data = rfdat.train,
                     ntree = 10000,
                     proximity = TRUE)
# assess model fit
pred.rf2k <- predict(rf2k, rfdat.test,'response')
pred.rf5k <- predict(rf5k, rfdat.test,'response')
pred.rf10k <- predict(rf10k, rfdat.test,'response')

table(observed = as.factor(deg.seset[,deg.seset$exptset.seahack=="test"]$deg.risk), 
      predicted = pred.rf2k)
#       predicted
#observed  0  1
#       0 20  2
#       1  1 21
cm.2k <- table(observed = as.factor(deg.seset[,deg.seset$exptset.seahack=="test"]$deg.risk), 
      predicted = pred.rf2k)

table(observed = as.factor(deg.seset[,deg.seset$exptset.seahack=="test"]$deg.risk), 
      predicted = pred.rf5k)
#       predicted
#observed  0  1
#       0 19  3
#       1  1 21
cm.5k <- table(observed = as.factor(deg.seset[,deg.seset$exptset.seahack=="test"]$deg.risk), 
      predicted = pred.rf5k)

table(observed = as.factor(deg.seset[,deg.seset$exptset.seahack=="test"]$deg.risk), 
      predicted = pred.rf10k)
#       predicted
#observed  0  1
#       0 19  3
#       1  1 21
cm.10k <- table(observed = as.factor(deg.seset[,deg.seset$exptset.seahack=="test"]$deg.risk), 
      predicted = pred.rf10k)

rf.returnlist <- list('rf2k.results'=list('fitmodel'=rf2k,'conf.matrix'=cm.2k),
                      'rf5k.results'=list('fitmodel'=rf5k,'conf.matrix'=cm.5k),
                      'rf10k.results'=list('fitmodel'=rf10k,'conf.matrix'=cm.10k))
save(rf.returnlist, file=paste0(data.dir, sys.sep, rf.resultlist.name))

#==============================
# Append results to std. table
#==============================
i2kgenes = as.data.frame(importance(rf2k))
i5kgenes = as.data.frame(importance(rf5k))
i10kgenes = as.data.frame(importance(rf10k))
colnames(i2kgenes) <- c("rf2ktrees_MeanDecNodeImp")
colnames(i5kgenes) <- c("rf5ktrees_MeanDecNodeImp")
colnames(i10kgenes) <- c("rf10ktrees_MeanDecNodeImp")
#i2kgenes = i2kgenes[order(match(rownames(i2kgenes), rownames(standtable))),]
#i5kgenes = i5kgenes[order(match(rownames(i5kgenes), rownames(standtable))),]
#i10kgenes = i10kgenes[order(match(rownames(i10kgenes), rownames(standtable))),]
identical(rownames(i2kgenes), rownames(standtable))
identical(rownames(i5kgenes), rownames(standtable))
identical(rownames(i10kgenes), rownames(standtable))

standtable <- cbind(standtable, cbind(i2kgenes, cbind(i5kgenes, i10kgenes)))
save(standtable, file = paste0(data.dir, sys.sep, stdtable.name))

#================================
# Random forest importance plots
#================================
jpeg(paste0(figs.dir, sys.sep, rf.plotcomposite.name),8,10,units="in",res=400)
par(oma=c(5,5,2,1),mfrow=c(3,1))
import.cutoff = 0.2
rfall <- list(rf2k, rf5k, rf10k)
rftitles <- c("2,000 trees", "5,000 trees", "10,000 trees")
for(i in 1:length(rfall)){
  bpi <- as.vector(round(importance(rfall[[i]]), 2))
  names(bpi) <- rowData(deg.seset)$gene.symbol
  bpi <- bpi[order(bpi)]
  bpi <- bpi[bpi>import.cutoff]
  barplot(bpi, las=2, cex.names = 0.7, main=rftitles[i])
  if(i == 2){
    mtext("Importance",side=2,line=4,cex=1)
  }
  if(i == 3){
    mtext(paste0("Top Features (>",import.cutoff," importance)"), side=1, line=7, cex=1)
  }
}
dev.off()

#====================================
# RF importance by lasso coefficient
#====================================
plot(standtable$lasso_coeff, standtable$rf2ktrees_MeanDecNodeImp)
plot(standtable$lasso_coeff, standtable$rf5ktrees_MeanDecNodeImp)
plot(standtable$lasso_coeff, standtable$rf10ktrees_MeanDecNodeImp)

#par(oma=c(7,3,3,3))
boxplot(standtable[,c("rf2ktrees_MeanDecNodeImp",
                      "rf5ktrees_MeanDecNodeImp",
                      "rf10ktrees_MeanDecNodeImp")],
        las=2)

hist(standtable$rf2ktrees_MeanDecNodeImp)

qrf2k_95quant = quantile(standtable$rf2ktrees_MeanDecNodeImp, seq(0,1,0.05))[20]
nrow(standtable[standtable$rf2ktrees_MeanDecNodeImp>=qrf2k_95quant,]) # 199 genes

nrow(standtable[standtable$rf2ktrees_MeanDecNodeImp>=qrf2k_95quant &
                  !standtable$lasso_coeff==0,]) # 6 genes

nrow(standtable[standtable$rf2ktrees_MeanDecNodeImp>=qrf2k_95quant &
                  (!standtable$lasso_coeff==0|
                     !standtable$lasso_rmCoeff==0),]) # 14 genes

```

## Random Forest with Boosting
Random forest was run again, this time with boosting.

```{r random_forest_withboost}
```

## AutoML
We futher tested results from running automated ensemble learning algorithm, AutoML.

```{r automl}
```

# Machine Learning: Checking Correlation with Retained Genes
# Correlational Map
Note: follow up with investigation of repetitions of lasso. Do reps show different predictors retained/eliminated? Are correlated genes correlated because they share pathway/network/annotation with selected predictor?

```{r corplot_lasso2runs}
# Make cor plot of top features selected by 
# initial lasso, and lasso subset 
# (excluding initial selected features)

# corr data, full dataset
cormat <- round(cor(t(assay(degfilt.se)), method="spearman"),3)
melted_cormat <- melt(cormat)
corhist = 
# corr table, filtered genes
goi <- rownames(standtable[!standtable$lasso_coeff==0 | !standtable$lasso_rmCoeff==0,])
cormat_filt <- round(cor(t(assay(degfilt.se[goi,])), method="spearman"),3)
melted_cormat_filt <- melt(cormat_filt)

#====================
# Density Histograms
#====================
corhist.name <- "corplot_hist_all-vs-lassogenes.jpg"
jpeg(paste0(figs.dir, sys.sep, corhist.name), 6,4,units="in",res=400)
plot(density(melted_cormat$value), col=rgb(0.2,0.2,0.8,alpha=0.2), lwd=2, lty=2,
     xlim=c(-1.2,1.2), ylab="Relative Density", xlab="Rho", main="Gene Expr. Correlation Dist.")
lines(density(melted_cormat_filt$value), col=rgb(0.1,0.9,0.2),lwd=3, lty=1)
legend("topright",bty="n",legend=c("All Genes", "Lasso Genes\n(2 reps, with and\nwithout coeff. filt)"), col=c(rgb(0.2,0.2,0.8,alpha=0.2), rgb(0.1,0.9,0.2)), lwd=c(2,3),lty=c(2,1),cex=0.5)
dev.off()

#=======================
# Correlation ggHeatmap
#=======================
colors <- colorRampPalette(c("blue", "green", "yellow", "red"))(42)
colnames(melted_cormat_filt) <- c("Gene Feature 1","Gene Feature 2","Rho")
corhm = ggplot(data = melted_cormat_filt, aes(x=melted_cormat_filt$`Gene Feature 1`, y=melted_cormat_filt$`Gene Feature 2`, fill=melted_cormat_filt$Rho)) + 
  geom_tile() +
  scale_fill_gradientn(colours = colors) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) + 
  ggtitle("Spearman Correlation Results\nLasso Selected Features\n(First run and CoeffFilt run)") +
  xlab("Gene Feature 1") +
  ylab("Gene Feature 2") +
  labs(fill="Rho")

corhm.name = "corplot_hm_lassogenes.jpg"
jpeg(paste0(figs.dir, sys.sep, corhm.name), 7, 7, units="in", res=400)
corhm + scale_colour_gradientn(colours=rainbow(4))
dev.off()

```

# Machine Learning: Predictive Features Consensus

# Results Summaries

# R Env log